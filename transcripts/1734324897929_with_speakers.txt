[0:00:00 - 0:00:31] Speaker SPEAKER_00:  improve the performance of their custom lillems by 2240X.  And that sounds unbelievable, but Amrohan from Ionio.  And with the last four years we have held dozens and dozens of companies, build and launch  AI and SaaS projects.  And in this video I want to go over exactly how sometimes smart teams make very small  silly mistakes which leads to degraded performance in production.
[0:00:32 - 0:00:40] Speaker SPEAKER_00:  We met the founder when they directly booked a call to our site.  And on the call they talked about how they already had a MVP working.
[0:00:40 - 0:01:32] Speaker SPEAKER_00:  They were self-serving the models for their use case.  The servers were located in somewhere in Europe.  And they were spending anywhere between $15 to $25 per day on hosting.  Which is somewhere around $5 to $700 per month.  So it was a smaller project in terms of scale.  But the major problem that they were facing was the amount of output.  The GPU utilization on their project was somewhere in the low 10 to 20%.  But yet they were only able to process 2 to 4,000 requests in an R, which is very low for context.  And that's when our engineer, a pronoun started to dig deeper.  We tried to understand how actually they were hosting the models.  And we understood there were 3 things which they were doing wrong.  The number one thing was they were hosting the model using something called Olamah.  So now Olamah is technically an LLM hosting tool.
[0:01:32 - 0:01:48] Speaker SPEAKER_00:  But Olamah is useful and used for local development and for smaller POCs.  You're not really supposed to use it in production.  But what had ended up happening in this particular example was the same workflow they were using for development.  We were being used in production.
[0:01:49 - 0:02:43] Speaker SPEAKER_00:  And VLLems is made to be used for production deployment.  But that's what we use in our production projects.  So that was the first solution.  Second thing that we discovered speaking with them was the GPU utilization.  So even though they were trying to get as much output from the model as possible,  the GPU utilization told a different story.  They were only using a quarter of their GPUs.  And if you are trying to optimize for output, that shouldn't be the case.  We said that they used multiple instances of the same model.  So instead of just running a single model and having it used just 10% of your GPU,  if you run four instances of the model with the VLLems setup,  then what you could do is use the entire GPU that you're actually paying for since you're paying  by the R. You would be able to use the entire GPU, be at 100% utilization,  and serve more users at the same time.  That was a second edition.  Bitlimbing multiple instances of the model.  Third last discovery that we made was they were using a GPU that was not really optimized  for inference.
[0:02:43 - 0:03:44] Speaker SPEAKER_00:  They were using a GPU which was more optimized for training.  And if they moved to something like the Nvidia 360 GPU with enough VDAM,  you would be able to do the second edition that is hosting multiple instances of the model.  Along with these three editions, we also sent him our own internal SOPs that we use for  deploying models with VLLems. And you might find them somewhere in this post or comments or  something or on our side. And with all of these three improvements, we could guarantee that they  would reach the benchmarks that we would expect from a 7 billion parameter model,  which is spending between $5-$1000 a month.  And the output would be on the conservative side at least 500 requests every single minute,  which is down 20 times faster. And that is on the conservative edge, just using the base  architecture that they were already using. But if we worked on the prompts, we quantized the models.  There's a chance that we could probably get that number down to even one and a half times that.  So yeah, I mean, that's pretty much it. That's pretty much it. I think I covered everything.
[0:03:45 - 0:04:08] Speaker SPEAKER_00:  right. They were using a custom aluminum. They were hosting a donor server, a self-hosting it on a  server. And they were concurrently calling the model as well. But they were just missing these  three things. I mean, input to these three things and we would get better performance. So yeah,  that everyone was happy that day. All was well. That's it. Thanks, bye.
[0:04:10 - 0:04:13] Speaker SPEAKER_00: 
[0:04:15 - 0:04:18] Speaker SPEAKER_00: 
