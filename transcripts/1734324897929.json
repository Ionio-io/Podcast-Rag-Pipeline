[
  {
    "start": "0:00:00",
    "end": "0:00:05",
    "text": "On one of our consulting calls, we suggested just three changes to a CTO,",
    "speaker": "Unknown"
  },
  {
    "start": "0:00:05",
    "end": "0:00:10",
    "text": "which helped them improve the performance of their custom LLMs by 2240x.",
    "speaker": "Unknown"
  },
  {
    "start": "0:00:10",
    "end": "0:00:15",
    "text": "And that sounds unbelievable, but I'm Rohan from Ioneo.",
    "speaker": "Unknown"
  },
  {
    "start": "0:00:15",
    "end": "0:00:21",
    "text": "And over the last four years, we have helped dozens and dozens of companies build and launch AI and SaaS projects.",
    "speaker": "Unknown"
  },
  {
    "start": "0:00:21",
    "end": "0:00:28",
    "text": "And in this video, I want to go over exactly how sometimes smart teams make very small,",
    "speaker": "Unknown"
  },
  {
    "start": "0:00:28",
    "end": "0:00:32",
    "text": "silly mistakes, which leads to degraded performance in production.",
    "speaker": "Unknown"
  },
  {
    "start": "0:00:32",
    "end": "0:00:36",
    "text": "So here's the backstory. We met the founder when they directly booked a call to our site.",
    "speaker": "Unknown"
  },
  {
    "start": "0:00:36",
    "end": "0:00:41",
    "text": "And on the call, they talked about how they already had a MVP working.",
    "speaker": "Unknown"
  },
  {
    "start": "0:00:41",
    "end": "0:00:45",
    "text": "They had built the POC with GPT-4, then they moved on to custom LLMs.",
    "speaker": "Unknown"
  },
  {
    "start": "0:00:45",
    "end": "0:00:51",
    "text": "They were self-serving the models for their use case. The servers were located in somewhere in Europe.",
    "speaker": "Unknown"
  },
  {
    "start": "0:00:51",
    "end": "0:00:57",
    "text": "And they were spending anywhere between $15 to $25 per day on hosting,",
    "speaker": "Unknown"
  },
  {
    "start": "0:00:57",
    "end": "0:01:03",
    "text": "which is somewhere around $500 to $700 per month. So it was a smaller project in terms of scale.",
    "speaker": "Unknown"
  },
  {
    "start": "0:01:03",
    "end": "0:01:07",
    "text": "But the major problem that they were facing was the amount of output.",
    "speaker": "Unknown"
  },
  {
    "start": "0:01:07",
    "end": "0:01:12",
    "text": "The GP utilization on their project was somewhere in the low 10 to 20 percent.",
    "speaker": "Unknown"
  },
  {
    "start": "0:01:12",
    "end": "0:01:18",
    "text": "But yet they were only able to process 2 to 4,000 requests in an hour, which is very low for context.",
    "speaker": "Unknown"
  },
  {
    "start": "0:01:18",
    "end": "0:01:22",
    "text": "And that's when our engineer Pranav started to dig deeper.",
    "speaker": "Unknown"
  },
  {
    "start": "0:01:22",
    "end": "0:01:24",
    "text": "We tried to understand how actually they were hosting the models.",
    "speaker": "Unknown"
  },
  {
    "start": "0:01:24",
    "end": "0:01:28",
    "text": "And we understood there were three things which they were doing wrong.",
    "speaker": "Unknown"
  },
  {
    "start": "0:01:28",
    "end": "0:01:32",
    "text": "The number one thing was they were hosting the model using something called OLAMA.",
    "speaker": "Unknown"
  },
  {
    "start": "0:01:32",
    "end": "0:01:40",
    "text": "So now OLAMA is technically an LLM hosting tool, but OLAMA is useful and used for local development and for smaller POCs.",
    "speaker": "Unknown"
  },
  {
    "start": "0:01:40",
    "end": "0:01:42",
    "text": "You're not really supposed to use it in production.",
    "speaker": "Unknown"
  },
  {
    "start": "0:01:42",
    "end": "0:01:49",
    "text": "But what had ended up happening in this particular example was the same workflow they were using for development were being used in production.",
    "speaker": "Unknown"
  },
  {
    "start": "0:01:49",
    "end": "0:01:53",
    "text": "One of our suggestions was moving from OLAMA to using something called VLMs.",
    "speaker": "Unknown"
  },
  {
    "start": "0:01:53",
    "end": "0:01:56",
    "text": "And VLMs is made to be used for production deployment.",
    "speaker": "Unknown"
  },
  {
    "start": "0:01:56",
    "end": "0:01:58",
    "text": "That's what we use in our production projects.",
    "speaker": "Unknown"
  },
  {
    "start": "0:01:58",
    "end": "0:02:00",
    "text": "So that was the first session.",
    "speaker": "Unknown"
  },
  {
    "start": "0:02:00",
    "end": "0:02:04",
    "text": "Second thing that we discovered, speaking with them, was the GPU utilization.",
    "speaker": "Unknown"
  },
  {
    "start": "0:02:04",
    "end": "0:02:10",
    "text": "So even though they were trying to get as much output from the model as possible, the GPU utilization told a different story.",
    "speaker": "Unknown"
  },
  {
    "start": "0:02:10",
    "end": "0:02:12",
    "text": "They were only using a quarter of their GPUs.",
    "speaker": "Unknown"
  },
  {
    "start": "0:02:12",
    "end": "0:02:15",
    "text": "And if you are trying to optimize for output, that shouldn't be the case.",
    "speaker": "Unknown"
  },
  {
    "start": "0:02:15",
    "end": "0:02:18",
    "text": "We suggested that they use multiple instances of the same model.",
    "speaker": "Unknown"
  },
  {
    "start": "0:02:19",
    "end": "0:02:26",
    "text": "So instead of just running a single model and having it use just 10% of your GPU, if you run four instances of the model with a VLM setup,",
    "speaker": "Unknown"
  },
  {
    "start": "0:02:26",
    "end": "0:02:30",
    "text": "then what you could do is use the entire GPU that you are actually paying for since you're paying by the hour.",
    "speaker": "Unknown"
  },
  {
    "start": "0:02:30",
    "end": "0:02:36",
    "text": "You would be able to use the entire GPU, be at 100% utilization and serve more users at the same time.",
    "speaker": "Unknown"
  },
  {
    "start": "0:02:36",
    "end": "0:02:37",
    "text": "That was the second suggestion.",
    "speaker": "Unknown"
  },
  {
    "start": "0:02:37",
    "end": "0:02:39",
    "text": "Deploying multiple instances of the model.",
    "speaker": "Unknown"
  },
  {
    "start": "0:02:39",
    "end": "0:02:45",
    "text": "Third last discovery that we made was they were using a GPU that was not really optimized for inference.",
    "speaker": "Unknown"
  },
  {
    "start": "0:02:45",
    "end": "0:02:47",
    "text": "They were using a GPU which was more optimized for training.",
    "speaker": "Unknown"
  },
  {
    "start": "0:02:47",
    "end": "0:02:56",
    "text": "And if they moved to something like the Nvidia 360 GPU with enough VRAM, you would be able to do the second solution that is hosting multiple instances of the model.",
    "speaker": "Unknown"
  },
  {
    "start": "0:02:56",
    "end": "0:03:03",
    "text": "Along with these three solutions, we also sent him our own internal SOPs that we use for deploying models with VLMs.",
    "speaker": "Unknown"
  },
  {
    "start": "0:03:03",
    "end": "0:03:08",
    "text": "And you might find them somewhere in this post or comments or something or on our site.",
    "speaker": "Unknown"
  },
  {
    "start": "0:03:08",
    "end": "0:03:16",
    "text": "And with all of these three improvements, we could guarantee that they would reach the benchmarks that we would expect from a 7 billion parameter model.",
    "speaker": "Unknown"
  },
  {
    "start": "0:03:16",
    "end": "0:03:19",
    "text": "Which is spending between 5 to a thousand dollars a month.",
    "speaker": "Unknown"
  },
  {
    "start": "0:03:19",
    "end": "0:03:27",
    "text": "And the output would be on the conservative side at least 500 requests every single minute, which is down 20 times faster.",
    "speaker": "Unknown"
  },
  {
    "start": "0:03:27",
    "end": "0:03:31",
    "text": "And that is on the conservative end, just using the base architecture that they were already using.",
    "speaker": "Unknown"
  },
  {
    "start": "0:03:31",
    "end": "0:03:39",
    "text": "But if we work on the prompts, we quantize the models, there's a chance that we could probably get that number down to even one and a half times that.",
    "speaker": "Unknown"
  },
  {
    "start": "0:03:39",
    "end": "0:03:41",
    "text": "So yeah, I mean, that's pretty much it.",
    "speaker": "Unknown"
  },
  {
    "start": "0:03:41",
    "end": "0:03:43",
    "text": "That's pretty much it.",
    "speaker": "Unknown"
  },
  {
    "start": "0:03:43",
    "end": "0:03:44",
    "text": "I think I covered everything.",
    "speaker": "Unknown"
  },
  {
    "start": "0:03:44",
    "end": "0:03:50",
    "text": "Yeah, so, you know, in this example, the client was actually someone who was doing 90% of everything right.",
    "speaker": "Unknown"
  },
  {
    "start": "0:03:50",
    "end": "0:03:57",
    "text": "They were using a custom LLM, they were hosting it on a server, self hosting it on a server, and they were concurrently calling the model as well.",
    "speaker": "Unknown"
  },
  {
    "start": "0:03:57",
    "end": "0:03:58",
    "text": "But they were just missing these three things.",
    "speaker": "Unknown"
  },
  {
    "start": "0:03:58",
    "end": "0:04:02",
    "text": "I mean input to these three things and boom, we would get better performance.",
    "speaker": "Unknown"
  },
  {
    "start": "0:04:02",
    "end": "0:04:06",
    "text": "So yeah, everyone was happy that day.",
    "speaker": "Unknown"
  },
  {
    "start": "0:04:06",
    "end": "0:04:07",
    "text": "All was well.",
    "speaker": "Unknown"
  },
  {
    "start": "0:04:07",
    "end": "0:04:08",
    "text": "That's it. Thanks. Bye.",
    "speaker": "Unknown"
  },
  {
    "start": "0:04:08",
    "end": "0:04:12",
    "text": "Yeah, if you need help with AI in your company, drop me a message.",
    "speaker": "Unknown"
  },
  {
    "start": "0:04:12",
    "end": "0:04:14",
    "text": "That's it. Thanks. Bye bye.",
    "speaker": "Unknown"
  }
]